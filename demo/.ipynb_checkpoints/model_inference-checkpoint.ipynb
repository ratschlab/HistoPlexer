{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb823ac",
   "metadata": {},
   "source": [
    "### Demo code for running inference using trained model and extract features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85aceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import torch \n",
    "import glob \n",
    "from PIL import Image\n",
    "from types import SimpleNamespace\n",
    "import json\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "root_code = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, root_code)\n",
    "\n",
    "from src.models.generator import unet_translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31cc665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update following as needed\n",
    "checkpoint_path = 'model/pytorch_model.pt' # path to trained translator model\n",
    "config_path = 'model/config.json' # path to corresponding config file from trained model\n",
    "device = 'cuda:0'\n",
    "img_path = None # path to he tile, if None then demo done on random numpy array\n",
    "features = {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcbe9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config file\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f, object_hook=lambda d: SimpleNamespace(**d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3373282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = unet_translator(\n",
    "    input_nc=config.input_nc,\n",
    "    output_nc=config.output_nc,\n",
    "    use_high_res=config.use_high_res,\n",
    "    use_multiscale=config.use_multiscale,\n",
    "    ngf=config.ngf,\n",
    "    depth=config.depth,\n",
    "    encoder_padding=config.encoder_padding,\n",
    "    decoder_padding=config.decoder_padding, \n",
    "    device=\"cpu\", \n",
    "    extra_feature_size=config.fm_feature_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3df3717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint and set model in eval model\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['trans_ema_state_dict']) # trans_state_dict\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ea4003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f091d4a56a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining hooks to get features from unet bottleneck\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        print(f\"Hook triggered for {name}!\")\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "        features[name] = output.detach()\n",
    "    return hook\n",
    "# global avg pooling \n",
    "gap = nn.AdaptiveAvgPool2d(1) \n",
    "# translator hook in center block, can also do \"translator_model.center_block[0].conv1\"\n",
    "model.center_block[0].register_forward_hook(get_features('feats_translator'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72dc637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform function for img\n",
    "def transform_np_to_tensor(np_img):\n",
    "        ''' Construct torch tensor from a numpy array\n",
    "        np_img: numpy array of shape [H,W,C]\n",
    "        returns a torch tensor with shape [C,H,W]\n",
    "        '''\n",
    "        np_img = np_img.transpose((2, 0, 1))\n",
    "        np_img = np.ascontiguousarray(np_img)\n",
    "        np_img = np_img // 255 # use this if img in range 0-255\n",
    "        torch_img = torch.from_numpy(np_img).float()\n",
    "        torch_img = torch_img.unsqueeze(dim=0)\n",
    "        return torch_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44569afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# load image and apply transformation\n",
    "if img_path: \n",
    "    image = Image.open(img_path)\n",
    "else: \n",
    "    image = np.full((256,256,3), 255)\n",
    "image = transform_np_to_tensor(image).to(device)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87e38000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook triggered for feats_translator!\n",
      "Output shape: torch.Size([1, 256, 1, 1])\n",
      "torch.Size([1, 11, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# inference using model; 3 pred imc images are generated with diff resolution -- use last one\n",
    "_, _, pred_imc = model(image)\n",
    "print(pred_imc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f97b5f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 11)\n"
     ]
    }
   ],
   "source": [
    "pred_imc = pred_imc.detach().cpu().numpy().squeeze(0).transpose((1, 2, 0))\n",
    "print(pred_imc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b39fb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract features from encoder part of translator unet model \n",
    "pred_imc_feats = gap(features['feats_translator']).squeeze().cpu().numpy()\n",
    "pred_imc_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca654fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eba74e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
